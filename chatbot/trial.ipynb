{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot trial langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph langchain_openai langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Google Gemini API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "load_dotenv()\n",
    "google_api_key= os.getenv('GEMINI_API_KEY')\n",
    "print(google_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring a virtual Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Open a local file in binary write mode\n",
    "    with open(\"Chinook.db\", \"wb\") as file:\n",
    "        # Write the content of the response (the file) to the local file\n",
    "        file.write(response.content)\n",
    "    print(\"File downloaded and saved as Chinook.db\")\n",
    "else:\n",
    "    print(f\"Failed to download the file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the langchain SQLDatabase function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Database tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n",
    "print(db.dialect)\n",
    "print(db.get_usable_table_names())\n",
    "db.run(\"SELECT * FROM Artist LIMIT 10;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure your VertexAI credentials are configured\n",
    "\n",
    "%pip install --upgrade --quiet langchain-google-genai pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring my Large Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Langchain and Gemini AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Helper functions for my AI Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda, RunnableWithFallbacks\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "def create_tool_node_with_fallback(tools: list) -> RunnableWithFallbacks[Any, dict]:\n",
    "    \"\"\"\n",
    "    Create a ToolNode with a fallback to handle errors and surface them to the agent.\n",
    "    \"\"\"\n",
    "    return ToolNode(tools).with_fallbacks(\n",
    "        [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n",
    "    )\n",
    "\n",
    "\n",
    "def handle_tool_error(state) -> dict:\n",
    "    error = state.get(\"error\")\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",\n",
    "                tool_call_id=tc[\"id\"],\n",
    "            )\n",
    "            for tc in tool_calls\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with the tools to get the AI response on the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.agent_toolkits import SQLDatabaseToolkit\n",
    "\n",
    "\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "tools = toolkit.get_tools()\n",
    "\n",
    "list_tables_tool = next(tool for tool in tools if tool.name == \"sql_db_list_tables\")\n",
    "get_schema_tool = next(tool for tool in tools if tool.name == \"sql_db_schema\")\n",
    "\n",
    "print(list_tables_tool.invoke(\"\"))\n",
    "\n",
    "print(get_schema_tool.invoke(\"Artist\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A tool function that will perform query from the AI Agent Against the real Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def db_query_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Execute a SQL query against the database and get back the result.\n",
    "    If the query is not correct, an error message will be returned.\n",
    "    If an error is returned, rewrite the query, check the query, and try again.\n",
    "    \"\"\"\n",
    "    result = db.run_no_throw(query)\n",
    "    if not result:\n",
    "        return \"Error: Query failed. Please rewrite your query and try again.\"\n",
    "    return result\n",
    "\n",
    "\n",
    "print(db_query_tool.invoke(\"SELECT * FROM Artist LIMIT 10;\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt LLm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making sure that the sql will work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function 1 that returns in a dictionary way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "import re\n",
    "\n",
    "\n",
    "# 2. Modified system prompt\n",
    "query_check_system = \"\"\"You are a SQL expert that ALWAYS responds using the db_query_tool.\n",
    "Format your response EXACTLY like this: {{\"query\": \"SQL_STATEMENT\"}}\n",
    "Never add explanations or formatting.\n",
    "ALWAYS RESPOND WITH RAW JSON FORMAT - NO MARKDOWN CODE BLOCKS.\n",
    "Example of valid output:{{\"query\": \"SELECT * FROM Table;\"}}\"\"\"\n",
    "\n",
    "# 3. Create PROPERLY STRUCTURED prompt template\n",
    "query_check_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", query_check_system),\n",
    "    (\"human\", \"Validate and correct this SQL: {user}\")  # Single input variable\n",
    "])\n",
    "class MarkdownJsonParser(JsonOutputParser):\n",
    "    def parse(self, text: str):\n",
    "        # Extract JSON from markdown code block\n",
    "        json_match = re.search(r'```json\\n(.*?)\\n```', text, re.DOTALL)\n",
    "        if json_match:\n",
    "            return super().parse(json_match.group(1))\n",
    "        return super().parse(text)  # Fallback to raw JSON parsing\n",
    "\n",
    "# Add to your chain\n",
    "\n",
    "# 4. Configure model with tool binding\n",
    "gemini_model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    api_key=google_api_key,\n",
    "    temperature=0\n",
    ").bind_tools([db_query_tool])\n",
    "\n",
    "# 5. Create the full chain\n",
    "query_check_chain = (\n",
    "    query_check_prompt |\n",
    "    gemini_model |\n",
    "    MarkdownJsonParser()\n",
    ")\n",
    "\n",
    "\n",
    "# 6. Invoke with CORRECT input format\n",
    "response = query_check_chain.invoke({\n",
    "    \"user\": \"SELECT * FROM Artist LIMIT 10;\"  # Match the template variable\n",
    "})\n",
    "\n",
    "print(response)\n",
    "print(response['query'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function 2 that returns the sql query in the content only from The AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Updated system prompt focusing on query checking only\n",
    "query_check_system = \"\"\"You are an expert SQL validator. Your ONLY task is to return a corrected SQL query when errors exist. \n",
    "\n",
    "### Absolute Rules:\n",
    "1. If the query is correct: Return original query EXACTLY as received.\n",
    "2. If incorrect: Return ONLY the corrected SQL query with:\n",
    "   - Proper NULL handling in NOT IN clauses\n",
    "   - UNION ALL instead of UNION where appropriate\n",
    "   - Inclusive BETWEEN ranges\n",
    "   - Resolved data type mismatches\n",
    "   - Valid quotation of identifiers\n",
    "   - Correct function argument counts\n",
    "   - Explicit type casting when needed\n",
    "   - Valid join conditions\n",
    "   - ORDER BY only when explicitly required\n",
    "\n",
    "### Output Protocol:\n",
    "- **STRICTLY FORBIDDEN:** Explanations, markdown, formatting, or text beyond the SQL\n",
    "- **STRICTLY FORBIDDEN:** Comments like \"Corrected query:\" or \"Here is:\"\n",
    "- **ONLY OUTPUT:** 1 raw SQL query with no encapsulation\n",
    "- If input is correct: Return input verbatim\n",
    "\n",
    "Failure to follow these instructions will result in incorrect output.\"\"\"\n",
    "\n",
    "# Create the prompt template\n",
    "# query_check_prompt = ChatPromptTemplate.from_messages(\n",
    "#     [(\"system\", query_check_system), (\"placeholder\", \"{messages}\")]\n",
    "# )\n",
    "\n",
    "# Initialize the latest Google Gemini AI model\n",
    "gemini_model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    api_key=google_api_key,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# 2. Create PROMPT TEMPLATE CHAIN\n",
    "query_check = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", query_check_system),\n",
    "    (\"human\", \"{user}\")  # Simplified placeholder\n",
    "]) | gemini_model\n",
    "\n",
    "# 3. Invoke PROPERLY with input formatting\n",
    "response = query_check.invoke({\n",
    "    \"user\": \"SELECT * FROM Artist LIMIT 10;\"  # Direct SQL input\n",
    "})\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Define the Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda, RunnableWithFallbacks\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "def create_tool_node_with_fallback(tools: list) -> RunnableWithFallbacks[Any, dict]:\n",
    "    \"\"\"\n",
    "    Create a ToolNode with a fallback to handle errors and surface them to the agent.\n",
    "    \"\"\"\n",
    "    # Print the tools being passed to the function\n",
    "    print(f\"Creating ToolNode with tools: {tools}\")\n",
    "    \n",
    "    # Create the ToolNode\n",
    "    tool_node = ToolNode(tools)\n",
    "    \n",
    "    # Print when ToolNode is successfully created\n",
    "    print(\"ToolNode created successfully.\")\n",
    "    \n",
    "    # Attach fallbacks and print the action\n",
    "    tool_node_with_fallbacks = tool_node.with_fallbacks(\n",
    "        [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n",
    "    )\n",
    "    \n",
    "    # Print when fallbacks are added\n",
    "    print(\"Fallbacks added to ToolNode.\")\n",
    "    \n",
    "    return tool_node_with_fallbacks\n",
    "\n",
    "\n",
    "\n",
    "def handle_tool_error(state) -> dict:\n",
    "    error = state.get(\"error\")\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",\n",
    "                tool_call_id=tc[\"id\"],\n",
    "            )\n",
    "            for tc in tool_calls\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal\n",
    "\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "\n",
    "\n",
    "# Define the state for the agent\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(State)\n",
    "print(State)\n",
    "\n",
    "\n",
    "# Add a node for the first tool call\n",
    "\n",
    "def first_tool_call(state: State) -> dict[str, list[AIMessage]]:\n",
    "    debug_output= {\n",
    "        \"messages\": [\n",
    "            AIMessage(\n",
    "                content=\"\",\n",
    "                tool_calls=[\n",
    "                    {\n",
    "                        \"name\": \"sql_db_list_tables\",\n",
    "                        \"args\": {},\n",
    "                        \"id\": \"tool_abcd123\",\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    print(\"DEBUG: first_tool_call output:\", debug_output)  # Print for debugging\n",
    "    return debug_output\n",
    "\n",
    "\n",
    "\n",
    "def model_check_query(state: State) -> dict[str, list[AIMessage]]:\n",
    "    \"\"\"\n",
    "    Use this tool to double-check if your query is correct before executing it.\n",
    "    \"\"\"\n",
    "    # Extract the last message from the state\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    # Debugging: Print the last message to verify\n",
    "    print(f\"Last message: {last_message}\")\n",
    "    \n",
    "    # Ensure the input to query_check is a valid dictionary (user should be a list with the last message)\n",
    "    query_input = {\"user\": [last_message]}  # Make sure this is a valid dictionary with a list of messages\n",
    "    \n",
    "    # Debugging: Check the structure of the input\n",
    "    print(f\"Query input (before invoking): {query_input}\")\n",
    "    \n",
    "    try:\n",
    "        # Invoke query_check with the correct dictionary format\n",
    "        query_result = query_check.invoke(query_input)\n",
    "        \n",
    "        # Debugging: Print the result of query_check.invoke\n",
    "        print(f\"Query result (after invoke): {query_result}\")\n",
    "        \n",
    "        # Ensure that the content is strictly a string and not a tuple or dict\n",
    "        query_result_str = str(query_result[\"content\"])  # Ensure content is a string\n",
    "        \n",
    "        # Return the result wrapped in the expected structure\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                AIMessage(content=query_result_str)  # Pass the result of query_check.invoke as content\n",
    "            ]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # Catch any errors and print them for debugging\n",
    "        print(f\"Error during query_check.invoke: {e}\")\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                AIMessage(content=f\"Error: {str(e)}\")  # Return error message if invocation fails\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "workflow.add_node(\"first_tool_call\", first_tool_call)\n",
    "\n",
    "# Add nodes for the first two tools\n",
    "workflow.add_node(\n",
    "    \"list_tables_tool\", create_tool_node_with_fallback([list_tables_tool])\n",
    ")\n",
    "workflow.add_node(\"get_schema_tool\", create_tool_node_with_fallback([get_schema_tool]))\n",
    "gemini_model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    api_key=google_api_key,\n",
    "    temperature=0\n",
    ")\n",
    "#  gemini_model = ChatGoogleGenerativeAI(\n",
    "#                 model=\"gemini-1.5-flash\",\n",
    "#                 api_key=google_api_key,\n",
    "#                 temperature=0\n",
    "#             ).bind_tools([db_query_tool])\n",
    "# Add a node for a model to choose the relevant tables based on the question and available tables\n",
    "model_get_schema = gemini_model.bind_tools(\n",
    "    [get_schema_tool]\n",
    ")\n",
    "print(\"Modal Schema:\",model_get_schema)\n",
    "workflow.add_node(\n",
    "    \"model_get_schema\",\n",
    "    lambda state: {\n",
    "        \"messages\": [model_get_schema.invoke(state[\"messages\"])],\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "# Describe a tool to represent the end state\n",
    "class SubmitFinalAnswer(BaseModel):\n",
    "    \"\"\"Submit the final answer to the user based on the query results.\"\"\"\n",
    "\n",
    "    final_answer: str = Field(..., description=\"The final answer to the user\")\n",
    "\n",
    "\n",
    "# Add a node for a model to generate a query based on the question and schema\n",
    "query_gen_system = \"\"\"You are a SQL expert with a strong attention to detail.\n",
    "\n",
    "Given an input question, output a syntactically correct SQLite query to run, then look at the results of the query and return the answer.\n",
    "\n",
    "DO NOT call any tool besides SubmitFinalAnswer to submit the final answer.\n",
    "\n",
    "When generating the query:\n",
    "\n",
    "Output the SQL query that answers the input question without a tool call.\n",
    "\n",
    "Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most 5 results.\n",
    "You can order the results by a relevant column to return the most interesting examples in the database.\n",
    "Never query for all the columns from a specific table, only ask for the relevant columns given the question.\n",
    "\n",
    "If you get an error while executing a query, rewrite the query and try again.\n",
    "\n",
    "If you get an empty result set, you should try to rewrite the query to get a non-empty result set. \n",
    "NEVER make stuff up if you don't have enough information to answer the query... just say you don't have enough information.\n",
    "\n",
    "If you have enough information to answer the input question, simply invoke the appropriate tool to submit the final answer to the user.\n",
    "\n",
    "DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\"\"\"\n",
    "query_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", query_gen_system), (\"human\", \"{messages}\")]\n",
    ")\n",
    "print(\"Query Gen Prompt:\",query_gen_prompt)\n",
    "#  gemini_model = ChatGoogleGenerativeAI(\n",
    "#                 model=\"gemini-1.5-flash\",\n",
    "#                 api_key=google_api_key,\n",
    "#                 temperature=0\n",
    "#             ).bind_tools([db_query_tool])\n",
    "query_gen = query_gen_prompt | gemini_model.bind_tools(\n",
    "    [SubmitFinalAnswer]\n",
    ")\n",
    "\n",
    "\n",
    "def query_gen_node(state: State):\n",
    "    print(\"State\",state)\n",
    "    message= query_gen.invoke(state)\n",
    "   \n",
    "    # message=response['query']\n",
    "\n",
    "    # Sometimes, the LLM will hallucinate and call the wrong tool. We need to catch this and return an error message.\n",
    "    tool_messages = []\n",
    "    if message.tool_calls:\n",
    "        for tc in message.tool_calls:\n",
    "            if tc[\"name\"] != \"SubmitFinalAnswer\":\n",
    "                tool_messages.append(\n",
    "                    ToolMessage(\n",
    "                        content=f\"Error: The wrong tool was called: {tc['name']}. Please fix your mistakes. Remember to only call SubmitFinalAnswer to submit the final answer. Generated queries should be outputted WITHOUT a tool call.\",\n",
    "                        tool_call_id=tc[\"id\"],\n",
    "                    )\n",
    "                )\n",
    "    else:\n",
    "        tool_messages = []\n",
    "    return {\"messages\": [message] + tool_messages}\n",
    "\n",
    "\n",
    "workflow.add_node(\"query_gen\", query_gen_node)\n",
    "\n",
    "# Add a node for the model to check the query before executing it\n",
    "workflow.add_node(\"correct_query\", model_check_query)\n",
    "\n",
    "# Add node for executing the query\n",
    "workflow.add_node(\"execute_query\", create_tool_node_with_fallback([db_query_tool]))\n",
    "\n",
    "\n",
    "# Define a conditional edge to decide whether to continue or end the workflow\n",
    "def should_continue(state: State) -> Literal[END, \"correct_query\", \"query_gen\"]:\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there is a tool call, then we finish\n",
    "    if getattr(last_message, \"tool_calls\", None):\n",
    "        return END\n",
    "    if last_message.content.startswith(\"Error:\"):\n",
    "        return \"query_gen\"\n",
    "    else:\n",
    "        return \"correct_query\"\n",
    "\n",
    "\n",
    "# Specify the edges between the nodes\n",
    "workflow.add_edge(START, \"first_tool_call\")\n",
    "workflow.add_edge(\"first_tool_call\", \"list_tables_tool\")\n",
    "workflow.add_edge(\"list_tables_tool\", \"model_get_schema\")\n",
    "workflow.add_edge(\"model_get_schema\", \"get_schema_tool\")\n",
    "workflow.add_edge(\"get_schema_tool\", \"query_gen\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"query_gen\",\n",
    "    should_continue,\n",
    ")\n",
    "workflow.add_edge(\"correct_query\", \"execute_query\")\n",
    "workflow.add_edge(\"execute_query\", \"query_gen\")\n",
    "\n",
    "# Compile the workflow into a runnable\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying the AI workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = app.invoke(\n",
    "    {\"messages\": [(\"user\", \"Which are the songs in the database with the artist Aerosmith?\")]},\n",
    ")\n",
    "json_str = messages[\"messages\"][-1].tool_calls[0][\"args\"][\"final_answer\"]\n",
    "json_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# Define the state for the agent\n",
    "class State(BaseModel):\n",
    "    messages: List[AIMessage] = []\n",
    "\n",
    "# Initialize Gemini model\n",
    "gemini_model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    api_key=google_api_key,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Describe a tool to represent the end state\n",
    "class SubmitFinalAnswer(BaseModel):\n",
    "    \"\"\"Submit the final answer to the user based on the query results.\"\"\"\n",
    "    final_answer: str = Field(..., description=\"The final answer to the user\")\n",
    "\n",
    "# Query generation system prompt\n",
    "query_gen_system = \"\"\"You are a SQL expert with a strong attention to detail.\n",
    "\n",
    "Given an input question, output a syntactically correct SQLite query to run, then look at the results of the query and return the answer.\n",
    "\n",
    "DO NOT call any tool besides SubmitFinalAnswer to submit the final answer.\n",
    "\n",
    "When generating the query:\n",
    "\n",
    "Output the SQL query that answers the input question without a tool call.\n",
    "\n",
    "Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most 5 results.\n",
    "You can order the results by a relevant column to return the most interesting examples in the database.\n",
    "Never query for all the columns from a specific table, only ask for the relevant columns given the question.\n",
    "\n",
    "If you get an error while executing a query, rewrite the query and try again.\n",
    "\n",
    "If you get an empty result set, you should try to rewrite the query to get a non-empty result set. \n",
    "NEVER make stuff up if you don't have enough information to answer the query... just say you don't have enough information.\n",
    "\n",
    "If you have enough information to answer the input question, simply invoke the appropriate tool to submit the final answer to the user.\n",
    "\n",
    "DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\"\"\"\n",
    "\n",
    "# Query generation function\n",
    "def query_gen_node(state: State):\n",
    "    if not state.messages:\n",
    "        return {\"messages\": [AIMessage(content=\"Error: No input provided for query generation.\")]}\n",
    "    \n",
    "    message = gemini_model.invoke(state.messages[-1].content)\n",
    "    return {\"messages\": [AIMessage(content=message)]}\n",
    "\n",
    "# Model to check query before execution\n",
    "def model_check_query(state: State):\n",
    "    if not state.messages:\n",
    "        return {\"messages\": [AIMessage(content=\"Error: No query provided for validation.\")]}\n",
    "    \n",
    "    return {\"messages\": [AIMessage(content=\"Query is valid.\")]}\n",
    "\n",
    "# Execution function\n",
    "def execute_query(state: State):\n",
    "    if not state.messages:\n",
    "        return {\"messages\": [AIMessage(content=\"Error: No query to execute.\")]}\n",
    "    \n",
    "    return {\"messages\": [AIMessage(content=\"Query executed successfully.\")]}\n",
    "\n",
    "# Workflow execution\n",
    "state = State(messages=[AIMessage(content=\"Fetch the latest 5 records from users table.\")])\n",
    "state = query_gen_node(state)\n",
    "state = model_check_query(state)\n",
    "state = execute_query(state)\n",
    "\n",
    "print(state)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
